# üìÇ Projeto de Regress√£o: Previs√£o de Risco de Cr√©dito

Este reposit√≥rio cont√©m um projeto de Machine Learning focado na previs√£o de risco de cr√©dito utilizando o dataset **German Credit Data**. O projeto implementa t√©cnicas de regress√£o, valida√ß√£o cruzada robusta e otimiza√ß√£o de hiperpar√¢metros para prever a viabilidade de empr√©stimos.

---

## üìù Descri√ß√£o do Projeto

O objetivo principal √© prever se um cliente de banco ser√° classificado como "bom" ou "ruim" para a concess√£o de cr√©dito. Embora o problema seja originalmente de classifica√ß√£o, este projeto explora a abordagem via **Regress√£o** (utilizando `RandomForestRegressor`) para analisar a probabilidade e a magnitude dos riscos associados a vari√°veis como idade, status financeiro e hist√≥rico de cr√©dito.

---

## üõ†Ô∏è Tecnologias e Bibliotecas

O projeto foi desenvolvido em **Python 3.x** utilizando as seguintes bibliotecas:

* **Manipula√ß√£o de Dados:** `pandas`, `numpy`
* **Machine Learning:** `scikit-learn` (Random Forest, GridSearchCV, Cross-Validation)
* **Balanceamento:** `imbalanced-learn` (SMOTE)
* **Visualiza√ß√£o:** `matplotlib`, `seaborn`

---

## üìä Metodologia

O desenvolvimento seguiu um pipeline rigoroso de Ci√™ncia de Dados:

1.  **Carregamento e Imputa√ß√£o:** Convers√£o de dados para formato num√©rico e tratamento de valores ausentes utilizando a estrat√©gia de moda (valor mais frequente).
2.  **Escalonamento:** Aplica√ß√£o de `StandardScaler` para garantir que todas as vari√°veis estivessem na mesma escala, facilitando a converg√™ncia do modelo.
3.  **Balanceamento com SMOTE:** > No dataset **German Credit Data**, o desbalanceamento entre clientes "bons" e "ruins" pode enviesar o modelo. Utilizamos o **SMOTE (Synthetic Minority Over-sampling Technique)** para criar novos exemplos sint√©ticos da classe minorit√°ria atrav√©s da interpola√ß√£o. Isso garante que o modelo aprenda as caracter√≠sticas dos clientes de alto risco em vez de apenas memorizar a classe majorit√°ria.



4.  **Valida√ß√£o Cruzada:** Implementa√ß√£o de 5-fold CV para validar a capacidade de generaliza√ß√£o e reduzir o risco de overfitting.
5.  **Otimiza√ß√£o de Hiperpar√¢metros:** Uso de `GridSearchCV` para encontrar a configura√ß√£o ideal de profundidade e n√∫mero de √°rvores.

---

## üìà Resultados Finais

Ap√≥s o ajuste fino, o modelo apresentou os seguintes indicadores de performance:

### M√©tricas de Avalia√ß√£o
| M√©trica | Valor |
| :--- | :---: |
| **Acur√°cia M√©dia (CV)** | 0.7911 |
| **Precis√£o** | 0.7909 |
| **Recall** | 0.7699 |
| **F1-Score** | 0.7803 |

### Melhores Hiperpar√¢metros Encontrados
* `max_depth`: 10
* `n_estimators`: 50
* `max_features`: 'sqrt'
* `min_samples_leaf`: 1
* `min_samples_split`: 2

### An√°lise de Res√≠duos
Durante a execu√ß√£o, s√£o gerados gr√°ficos para validar a qualidade das previs√µes:
* **Res√≠duos vs. Valores Reais:** Verifica se os erros s√£o aleat√≥rios (ideal) ou se seguem um padr√£o (indicando falha do modelo).
* **Erro Absoluto:** Mede a magnitude m√©dia dos desvios em rela√ß√£o ao valor real.
